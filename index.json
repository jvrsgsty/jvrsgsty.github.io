[{"authors":["javier"],"categories":null,"content":"I am a PhD Candidate at the Institute for Computational and Mathematical Engineering at Stanford University. I joined the Stanford Neuroscience and Artificial Intelligence Laboratory with P.I. Dan Yamins in September 2018 interested in biologically-inspired computational intelligence. My interests grew to include using computational models to understand how the brain works, along with recurrent models of the visual system, learning rules, and deep learning theory.\nBefore starting grad school, I spent two years working at a Mexican FinTech startup, teaching Computer Science, and doing research in machine learning for text mining. My non-academic interests include alpine skiing, cycling, hiking, cooking, and an ever-increasing obsession with coffee.\n sagas [at] hey [dot] com\n","date":1597249156,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1597249156,"objectID":"b8a4159850cad0aa594097fa6c0b28d1","permalink":"https://www.javiersagastuy.com/author/javier-sagastuy-brena/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/javier-sagastuy-brena/","section":"authors","summary":"I am a PhD Candidate at the Institute for Computational and Mathematical Engineering at Stanford University. I joined the Stanford Neuroscience and Artificial Intelligence Laboratory with P.I. Dan Yamins in September 2018 interested in biologically-inspired computational intelligence.","tags":null,"title":"Javier Sagastuy-Brena","type":"authors"},{"authors":["Javier Sagastuy-Brena"],"categories":[],"content":"Review Summary  SGD performs similarly for different batch sizes, but a constant LR/BS ratio. The authors note that SGD with the same LR/BS ratio are different discretizations of the same Stochastic Differential Equation. LR schedules and BS schedules are interchangeable, what matters, again, is what the LR/BS looks like. Width of minima is defined in terms of the trace of the Hessian $Tr(H)$ at the minima: lower trace = wider minima.  Assumption 1: At a local minima, loss surface approximated via a quadratic bowl. This lets training process be approximated by Orenstein-Unhlenbcek process. Assumption 2: $H$ is approximated via the covariance matrix of the stochastic gradients ($H=C$ relies on $C$ being anisotropic).   Larger LR/BS correlates with wider minima, giving better generalization. However, larger $\\beta$, with constant $\\frac{LR}{BS}=\\frac{\\beta \\eta}{\\beta S}$ causes the approximation to the SDE to break down, leading to lower performance. Discretization errors become aparent at large learning rates. Central Limit Theorem assumptions break down for small dataset, large batches.  ","date":1597249156,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597249156,"objectID":"98fd0defdc7e92435ed20835e0e78484","permalink":"https://www.javiersagastuy.com/review/200812_three-factors-influencing-minima/","publishdate":"2020-08-12T09:19:16-07:00","relpermalink":"/review/200812_three-factors-influencing-minima/","section":"review","summary":"Review Summary  SGD performs similarly for different batch sizes, but a constant LR/BS ratio. The authors note that SGD with the same LR/BS ratio are different discretizations of the same Stochastic Differential Equation.","tags":["Learning Dynamics","Theory","SGD"],"title":"Three Factors Influencing Minima in SGD","type":"review"},{"authors":["Javier Sagastuy-Brena"],"categories":[],"content":"Review Summary  Batch normalization induces scale invariance of the loss with respect to the weights (i.e. $L(x; \\theta) = L(x; k \\theta)$, for parameters $\\theta$ with BN. This expression is not mathematically precise). The scale invariance implies in $\u0026lt;\\theta_t, \\partial L / \\partial \\theta |_{\\theta = \\theta_t}\u0026gt; = 0$ A DNN trained with BN and WD using SDGM, with WD $\\lambda$, Momentum $\\alpha \\geq 0$ and LR $\\eta$ has an angular update $\\Delta_t$ satisfying $$ \\lim_{t\\rightarrow \\infty}\\Delta_t \\simeq \\sqrt{\\dfrac{2\\eta\\lambda}{1+\\alpha}} $$ At equilibrium, the \u0026ldquo;force\u0026rdquo; pulling the weights towards the origin (from the weight decay term) and the \u0026ldquo;force\u0026rdquo; pushing the weights orthogonally balance out, resulting in movement on the surface of a sphere, with a fixed angular update at every step in training. Properties of Spherical Motion Dynamics:  Scale-invariant weights do not suffer from vanishing or exploding gradients. SGDM cannot get trapped in local minima, since it guarantees an angular update size on $\\mathcal{S}^{p-1}$. With SGDM weights will not converge unless the LR $\\eta$ is manually decreased.    Comments   The theory shown does not seem to consider the role of the batch size in the stochasticity of the training process.\n  The authors\u0026rsquo; discussion on the Linear Scaling Principle (rescaling the learning rate with the batch size) is not entirely clear to me. It seems the main takeaway is that when rescaling the LR by a factor of $k$ to use a batch size that is $k$ times larger, then the angular update is affected mainly by the change in the rescaling of the LR (which scales with $\\sqrt k$).\n The authors compare the angular update between $w_t$ and $w_{t+1}$ for the larger batch size against the cumulative angular update over $k$ steps for the smaller batch. We interpret this to mean the angular update between $w_t$ and $w_{t+k}$, since we cannot say anything about the additivity of the angular updates. The cummulative angular update for small batch is larger than the single step angular update for larger batch size. In fact, we can approximately observe:  $$ \\dfrac{\\Delta_t^{256}(w_t, w_{t+k})}{\\Delta_t^{k256}(w_t, w_{t+1})} \\lessapprox \\sqrt k = \\dfrac{\\sqrt{2\\eta\\lambda/(1+\\alpha)}}{\\sqrt{2k\\eta\\lambda/(1+\\alpha)}} $$\n  ","date":1596298756,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596298756,"objectID":"498ec3b21655a0c76a16d0599276b714","permalink":"https://www.javiersagastuy.com/review/200724_spherical-motion-dynamics/","publishdate":"2020-08-12T09:19:16-07:00","relpermalink":"/review/200724_spherical-motion-dynamics/","section":"review","summary":"Review Summary  Batch normalization induces scale invariance of the loss with respect to the weights (i.e. $L(x; \\theta) = L(x; k \\theta)$, for parameters $\\theta$ with BN. This expression is not mathematically precise).","tags":["Learning Dynamics","Theory"],"title":"Spherical Motion Dynamics of Deep Neural Networks with Batch Normalization and Weight Decay","type":"review"},{"authors":["Daniel Kunin","Aran Nayebi","Javier Sagastuy-Brena","Surya Ganguli","Jon Bloom","Daniel LK Yamins"],"categories":null,"content":"","date":1583020800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583020800,"objectID":"f759b3e069d311e023061c62ef9f613b","permalink":"https://www.javiersagastuy.com/publication/kunin-2020-two/","publishdate":"2020-07-09T20:23:32.256661Z","relpermalink":"/publication/kunin-2020-two/","section":"publication","summary":"The neural plausibility of backpropagation has long been disputed, primarily for its use of non-local weight transport - the biologically dubious requirement that one neuron instantaneously measure the synaptic weights of another. Until recently, attempts to create local learning rules that avoid weight transport have typically failed in the large-scale learning scenarios where backpropagation shines, e.g. ImageNet categorization with deep convolutional networks. Here, we investigate a recently proposed local learning rule that yields competitive performance with backpropagation and find that it is highly sensitive to metaparameter choices, requiring laborious tuning that does not transfer across network architecture. Our analysis indicates the underlying mathematical reason for this instability, allowing us to identify a more robust local learning rule that better transfers without metaparameter tuning. Nonetheless, we find a performance and stability gap between this local rule and backpropagation that widens with increasing model depth. We then investigate several non-local learning rules that relax the need for instantaneous weight transport into a more biologically-plausible 'weight estimation' process, showing that these rules match state-of-the-art performance on deep networks and operate effectively in the presence of noisy updates. Taken together, our results suggest two routes towards the discovery of neural implementations for credit assignment without weight symmetry: further improvement of local rules so that they perform consistently across architectures and the identification of biological implementations for non-local learning mechanisms.","tags":["ICML","Learning Rules","Deep Learning"],"title":"Two Routes to Scalable Credit Assignment without Weight Symmetry","type":"publication"}]