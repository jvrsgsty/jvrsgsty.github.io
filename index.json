[{"authors":["javier"],"categories":null,"content":"I am a PhD Candidate at the Institute for Computational and Mathematical Engineering at Stanford University. I joined the Stanford Neuroscience and Artificial Intelligence Laboratory with P.I. Dan Yamins in September 2018 widely interested in understanding how the human brain works using computational models. I\u0026rsquo;ve worked in projects on recurrent models of the visual system, biologically-inspired learning rules and deep learning theory. More recently I\u0026rsquo;ve been working on understanding how to measure similarity betweeen individuals\u0026rsquo; neural responses and how to map between them. I am also interested in the application and safe deployment of AI systems in the real world.\nBefore starting grad school, I spent two years working at a Mexican FinTech startup, teaching Computer Science, and doing research in machine learning for text mining. My non-academic interests include alpine skiing, cycling, hiking, cooking, and an ever-increasing obsession with coffee.\n sagas [at] hey [dot] com\n","date":1661385600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1661385600,"objectID":"b8a4159850cad0aa594097fa6c0b28d1","permalink":"https://www.javiersagastuy.com/author/javier-sagastuy-brena/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/javier-sagastuy-brena/","section":"authors","summary":"I am a PhD Candidate at the Institute for Computational and Mathematical Engineering at Stanford University. I joined the Stanford Neuroscience and Artificial Intelligence Laboratory with P.I. Dan Yamins in September 2018 widely interested in understanding how the human brain works using computational models.","tags":null,"title":"Javier Sagastuy-Brena","type":"authors"},{"authors":["Javier Sagastuy-Brena","Imran Thobani","Aran Nayebi","Rosa Cao","Daniel LK Yamins"],"categories":null,"content":"","date":1661385600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661385600,"objectID":"ba15c524bfba66e25f1ce37118f565ce","permalink":"https://www.javiersagastuy.com/publication/sagastuy-2022-interanimal/","publishdate":"2022-08-25T20:23:32.256661Z","relpermalink":"/publication/sagastuy-2022-interanimal/","section":"publication","summary":"Accurately measuring similarity between different animalsâ€™ neural responses is a crucial step towards evaluating deep neural network (DNN) models of the brain. Under what transform class are animals likely to be similar to each other, and how much neural data needs to be collected to get an accurate similarity estimate? Using model variability as a proxy for inter-animal variability, we find that where we measure similarity from has critical implications for the suitable transform class. Specifically, we observe high linear mappability between pre-ReLU activations, but require a simple non-linear mapping class (that combines logistic regression with linear regression) in the case of post-ReLU activations. With our approach, we estimate that measuring inter-animal variability requires collecting neural data for at least 500 stimuli and 300 neurons from the same hypercolumn, providing a prescription for future experimental data that can adjudicate between models.","tags":["Neural Similarity","Similarity","Visual System","Mouse"],"title":"Modelling inter-animal variability","type":"publication"},{"authors":["Aran Nayebi","Javier Sagastuy-Brena","Daniel M Bear","Kohitij Kar","Jonas Kubilius","Surya Ganguli","David Sussillo","James J DiCarlo","Daniel LK Yamins"],"categories":null,"content":"","date":1657756800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1657756800,"objectID":"28a2331f39789939ec79f89e425f196d","permalink":"https://www.javiersagastuy.com/publication/nayebi-2022-recurrent/","publishdate":"2022-07-14T20:23:32.256661Z","relpermalink":"/publication/nayebi-2022-recurrent/","section":"publication","summary":"The computational role of the abundant feedback connections in the ventral visual stream is unclear, enabling humans and nonhuman primates to effortlessly recognize objects across a multitude of viewing conditions. Prior studies have augmented feedforward convolutional neural networks (CNNs) with recurrent connections to study their role in visual processing; however, often these recurrent networks are optimized directly on neural data or the comparative metrics used are undefined for standard feedforward networks that lack these connections. In this work, we develop task-optimized convolutional recurrent (ConvRNN) network models that more correctly mimic the timing and gross neuroanatomy of the ventral pathway. Properly chosen intermediate-depth ConvRNN circuit architectures, which incorporate mechanisms of feedforward bypassing and recurrent gating, can achieve high performance on a core recognition task, comparable to that of much deeper feedforward networks. We then develop methods that allow us to compare both CNNs and ConvRNNs to finely grained measurements of primate categorization behavior and neural response trajectories across thousands of stimuli. We find that high-performing ConvRNNs provide a better match to these data than feedforward networks of any depth, predicting the precise timings at which each stimulus is behaviorally decoded from neural activation patterns. Moreover, these ConvRNN circuits consistently produce quantitatively accurate predictions of neural dynamics from V4 and IT across the entire stimulus presentation. In fact, we find that the highest-performing ConvRNNs, which best match neural and behavioral data, also achieve a strong Pareto trade-off between task performance and overall network size. Taken together, our results suggest the functional purpose of recurrence in the ventral pathway is to fit a high-performing network in cortex, attaining computational power through temporal rather than spatial complexity.","tags":["RNN","CNN","Deep Learning","Visual System","Primate"],"title":"Recurrent Connections in the Primate Ventral Visual Stream Mediate a Trade-Off Between Task Performance and Network Size During Core Object Recognition","type":"publication"},{"authors":["Daniel Kunin","Javier Sagastuy-Brena","Lauren Gillespie","Eshed Margalit","Hidenori Tanaka","Surya Ganguli","Daniel LK Yamins"],"categories":null,"content":"","date":1626652800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1626652800,"objectID":"5ca00ced0a9d83d851389a05acb1cdd7","permalink":"https://www.javiersagastuy.com/publication/kunin-2021-limiting/","publishdate":"2021-07-19T20:23:32.256661Z","relpermalink":"/publication/kunin-2021-limiting/","section":"publication","summary":"In this work we explore the limiting dynamics of deep neural networks trained with stochastic gradient descent (SGD). As observed previously, long after performance has converged, networks continue to move through parameter space by a process of anomalous diffusion in which distance travelled grows as a power law in the number of gradient updates with a nontrivial exponent. We reveal an intricate interaction between the hyperparameters of optimization, the structure in the gradient noise, and the Hessian matrix at the end of training that explains this anomalous diffusion. To build this understanding, we first derive a continuous-time model for SGD with finite learning rates and batch sizes as an underdamped Langevin equation. We study this equation in the setting of linear regression, where we can derive exact, analytic expressions for the phase space dynamics of the parameters and their instantaneous velocities from initialization to stationarity. Using the Fokker-Planck equation, we show that the key ingredient driving these dynamics is not the original training loss, but rather the combination of a modified loss, which implicitly regularizes the velocity, and probability currents, which cause oscillations in phase space. We identify qualitative and quantitative predictions of this theory in the dynamics of a ResNet-18 model trained on ImageNet. Through the lens of statistical physics, we uncover a mechanistic origin for the anomalous limiting dynamics of deep neural networks trained with SGD. ","tags":["Deep Learning","Theory"],"title":"Limiting Dynamics of SGD: Modified Loss, Phase Space Oscillations, and Anomalous Diffusion","type":"publication"},{"authors":["Javier Sagastuy-Brena"],"categories":[],"content":"Review Summary  The authors build on ideas from the statistical shape analysis literature to develop similarity measures that are metrics: that are positive, symmetric and respect the triangle inequality. They highlight how quantifying the similatiry between matrices of high dimensional vectors in $\\mathbb R^n$ over a set of $m$ reference stimuli is complicated. Previous approaches include PLS regressiom (which is asymmetric), CCA, CKA, and RSA (which are symmetric, but do not satisfy the triangle inequality), among others. The authors define representations as equivalent if they are linearly mappable to each other within a set of allowable transformations $\\mathcal G$, and highlight that nonlinear mappings may be achieved by preprocessing the features with a nonlinear function. $\\mathcal G$ is normally the set of orthogonal transforms with determinant of 1, but here the authors generalize this to suit the use case of quantifying dissimilarity in neural representations.  In particular, they propose $\\mathcal G$ be the group of linear isometries, i.e. distance-preserving transforms, which turns out to be a subgroup of the set of orthogonal transformations (without the det = 1 condition).   Dependig on the desired notion of representational simlilarity, the authors exhibit how to define certain metrics where similarity can be assesed:  Up to permutation (linear ssignment problem) Up to rotation (orthogonal Procrustes problem) Up to a an affine transform (related to CCA) Up to nonlinear transforms (in the appendix, related to CKA)   The authors also define a metric for convolutional layers that leverages the spatial structure of conv layers and relies the group of circular shifts and Kronecker products. The authors present some experimental results on neural mouse data and NAS-Bench-101 data:  They show how trinagle inequality violations occur in practice (though not always, depending on the dataset) for commonly used simmilarity measures, and that generalized shape metrics correct for this. They show that neural representation metric spaces can be embeded in Euclidean spaces but suggest using a higher number of embedding dimensions than what is normally used. They show such embeddings of neural data preserve neural hierarchy structure and are scientifically interpretable. They show that in artificial NNs, the representations show consistent structure across layers.    Comments    ","date":1612714756,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1612714756,"objectID":"7adc547145986d9f44328375b1ff0f20","permalink":"https://www.javiersagastuy.com/review/220201_generalized-shape-metrics-neural/","publishdate":"2022-02-07T09:19:16-07:00","relpermalink":"/review/220201_generalized-shape-metrics-neural/","section":"review","summary":"Review Summary  The authors build on ideas from the statistical shape analysis literature to develop similarity measures that are metrics: that are positive, symmetric and respect the triangle inequality. They highlight how quantifying the similatiry between matrices of high dimensional vectors in $\\mathbb R^n$ over a set of $m$ reference stimuli is complicated.","tags":["Neural Representations","Metrics","Representation Similarity"],"title":"Generalized Shape Metrics on Neural Representations","type":"review"},{"authors":["Daniel Kunin","Javier Sagastuy-Brena","Surya Ganguli","Daniel LK Yamins","Hidenori Tanaka"],"categories":null,"content":"","date":1607472000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607472000,"objectID":"112e579819ef23a334ecabbb3d7cf9f0","permalink":"https://www.javiersagastuy.com/publication/kunin-2020-neural/","publishdate":"2020-12-09T20:23:32.256661Z","relpermalink":"/publication/kunin-2020-neural/","section":"publication","summary":"Predicting the dynamics of neural network parameters during training is one of the key challenges in building a theoretical foundation for deep learning. A central obstacle is that the motion of a network in high-dimensional parameter space undergoes discrete finite steps along complex stochastic gradients derived from real-world datasets. We circumvent this obstacle through a unifying theoretical framework based on intrinsic symmetries embedded in a network's architecture that are present for any dataset. We show that any such symmetry imposes stringent geometric constraints on gradients and Hessians, leading to an associated conservation law in the continuous-time limit of stochastic gradient descent (SGD), akin to Noether's theorem in physics. We further show that finite learning rates used in practice can actually break these symmetry induced conservation laws. We apply tools from finite difference methods to derive modified gradient flow, a differential equation that better approximates the numerical trajectory taken by SGD at finite learning rates. We combine modified gradient flow with our framework of symmetries to derive exact integral expressions for the dynamics of certain parameter combinations. We empirically validate our analytic predictions for learning dynamics on VGG-16 trained on Tiny ImageNet. Overall, by exploiting symmetry, our work demonstrates that we can analytically describe the learning dynamics of various parameter combinations at finite learning rates and batch sizes for state of the art architectures trained on any dataset. ","tags":["Deep Learning","Theory","Symmetry","Conservation Laws"],"title":"Neural Mechanics: Symmetry and Broken Conservation Laws in Deep Learning Dynamics","type":"publication"},{"authors":["Javier Sagastuy-Brena"],"categories":[],"content":"Review Summary Variantional inference  Proof that SGD minimizes a potential along with an entropic regularization term. However, this potential differs from the loss used to compute backpropagation gradients. They are only equal if the gradient noise were isotropic (i.e. a constant multiple of the identity). The gradient noise is highly non-isotropic with a very low rank covariance matrix. The locations in weight space to which SGD \u0026ldquo;converges\u0026rdquo; are not true local minima of the original loss: Their SDE analysis does not make the same assumptions as previous papers (e.g. quadratic approximation of the loss), even though the SDE they analyze is the same as in the papers I recently reviewed (albeit with different notation). Instead, they show how the continuous-time limit of SGD evolves according to the Fokker-Planck equation.  No quadratic assumption on loss No constant covariance (thought experiments show it does not change much)   The practical implications of their analysis are:  The LR should scale linearly with BS to generalize well. Sampling with replacement is better than without replacement, since the diffusion matrix (i.e. correlation matrix), with a more prononced effect at larger batches.    Limit Cycles  In a vicinity of a critical point, the weights do not present Brownian motion, rather they have a deterministic component generating loops in weight space. Relevant observables shown:  Eigenvalue spectra of covariance matrices over time (does not change much), along with mean and std of eigenvalues, as well as rank. These quantities inform the performance of a given architecture on a dataset. Fast Fourier Transform of weights over time (should be constant in Brownian motion case). Auto-correlation of weights (should quickly decay to zero in Brownian motion case). Dimension-normalized gradient norm: they observe it does not decrease wrt the number of epochs and even shows a growing trend. This is justified by the presence of a non-zero force $j(x)$ which is responsible for theobserved Fourier harmonics.    ","date":1598667556,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598667556,"objectID":"751a80def7e9f05a5ec2bc2031fca129","permalink":"https://www.javiersagastuy.com/review/200828_sg-performs-variational-inference-limit-cycles/","publishdate":"2020-08-18T09:19:16-07:00","relpermalink":"/review/200828_sg-performs-variational-inference-limit-cycles/","section":"review","summary":"Review Summary Variantional inference  Proof that SGD minimizes a potential along with an entropic regularization term. However, this potential differs from the loss used to compute backpropagation gradients. They are only equal if the gradient noise were isotropic (i.","tags":["Learning Dynamics","Theory","SDE"],"title":"Stochastic gradient descent performs variational inference, converges to limit cycles for deep networks","type":"review"},{"authors":["Javier Sagastuy-Brena"],"categories":[],"content":"Review Summary  The authors expand on their previous work on the continuous time-limit of SGD. They show how SGD with a constant LR can be modelled as an SDE that reaches a stationary distribution. Further, if the loss function is interpreted as a log-posterior and make the goal to approximate the posterior as well as possible (in terms of KL divergence), the authors derive optimal hyperparameters. 4 key assumptions:  The gradient noise is Gaussian The weight estimates are constrained to a small enough region of parameter space that the sampling noise covariance of the stochastic gradients is constant. The step size is small enough that the discrete Markov Process can be approximated by a continuous time Markov process. Lastly, the stationary distribution of the process has a closed form solution if we assume we can approximate the loss as a quadratic bowl (we are close to a local minima).   The authors show how this SDE view is related to stochastic gradient Fisher scoring. The main practical contribution, is that this view provides a way to compute mathematically optimal hyperparameters for a small amount if data, in contrast to the computational burden of cross-validating parameters.  ","date":1597767556,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597767556,"objectID":"ac7d81c69de98b449864805b014ff96e","permalink":"https://www.javiersagastuy.com/review/200818_variational-analysis-sgd/","publishdate":"2020-08-18T09:19:16-07:00","relpermalink":"/review/200818_variational-analysis-sgd/","section":"review","summary":"Review Summary  The authors expand on their previous work on the continuous time-limit of SGD. They show how SGD with a constant LR can be modelled as an SDE that reaches a stationary distribution.","tags":["Learning Dynamics","Theory"],"title":"A Variational Analysis of Stochastic Gradient Algorithms","type":"review"},{"authors":["Javier Sagastuy-Brena"],"categories":[],"content":"Review Summary  SGD performs similarly for different batch sizes, but a constant LR/BS ratio. The authors note that SGD with the same LR/BS ratio are different discretizations of the same Stochastic Differential Equation. LR schedules and BS schedules are interchangeable, what matters, again, is what the LR/BS looks like. Width of minima is defined in terms of the trace of the Hessian $Tr(H)$ at the minima: lower trace = wider minima.  Assumption 1: At a local minima, loss surface approximated via a quadratic bowl. This lets training process be approximated by Orenstein-Unhlenbcek process. Assumption 2: $H$ is approximated via the covariance matrix of the stochastic gradients ($H=C$ relies on $C$ being anisotropic).   Larger LR/BS correlates with wider minima, giving better generalization. However, larger $\\beta$, with constant $\\frac{LR}{BS}=\\frac{\\beta \\eta}{\\beta S}$ causes the approximation to the SDE to break down, leading to lower performance. Discretization errors become aparent at large learning rates. Central Limit Theorem assumptions break down for small dataset, large batches.  ","date":1597249156,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597249156,"objectID":"98fd0defdc7e92435ed20835e0e78484","permalink":"https://www.javiersagastuy.com/review/200812_three-factors-influencing-minima/","publishdate":"2020-08-12T09:19:16-07:00","relpermalink":"/review/200812_three-factors-influencing-minima/","section":"review","summary":"Review Summary  SGD performs similarly for different batch sizes, but a constant LR/BS ratio. The authors note that SGD with the same LR/BS ratio are different discretizations of the same Stochastic Differential Equation.","tags":["Learning Dynamics","Theory","SGD"],"title":"Three Factors Influencing Minima in SGD","type":"review"},{"authors":["Javier Sagastuy-Brena"],"categories":[],"content":"Review Summary  Batch normalization induces scale invariance of the loss with respect to the weights (i.e. $L(x; \\theta) = L(x; k \\theta)$, for parameters $\\theta$ with BN. This expression is not mathematically precise). The scale invariance implies in $\u0026lt;\\theta_t, \\partial L / \\partial \\theta |_{\\theta = \\theta_t}\u0026gt; = 0$ A DNN trained with BN and WD using SDGM, with WD $\\lambda$, Momentum $\\alpha \\geq 0$ and LR $\\eta$ has an angular update $\\Delta_t$ satisfying $$ \\lim_{t\\rightarrow \\infty}\\Delta_t \\simeq \\sqrt{\\dfrac{2\\eta\\lambda}{1+\\alpha}} $$ At equilibrium, the \u0026ldquo;force\u0026rdquo; pulling the weights towards the origin (from the weight decay term) and the \u0026ldquo;force\u0026rdquo; pushing the weights orthogonally balance out, resulting in movement on the surface of a sphere, with a fixed angular update at every step in training. Properties of Spherical Motion Dynamics:  Scale-invariant weights do not suffer from vanishing or exploding gradients. SGDM cannot get trapped in local minima, since it guarantees an angular update size on $\\mathcal{S}^{p-1}$. With SGDM weights will not converge unless the LR $\\eta$ is manually decreased.    Comments   The theory shown does not seem to consider the role of the batch size in the stochasticity of the training process.\n  The authors\u0026rsquo; discussion on the Linear Scaling Principle (rescaling the learning rate with the batch size) is not entirely clear to me. It seems the main takeaway is that when rescaling the LR by a factor of $k$ to use a batch size that is $k$ times larger, then the angular update is affected mainly by the change in the rescaling of the LR (which scales with $\\sqrt k$).\n The authors compare the angular update between $w_t$ and $w_{t+1}$ for the larger batch size against the cumulative angular update over $k$ steps for the smaller batch. We interpret this to mean the angular update between $w_t$ and $w_{t+k}$, since we cannot say anything about the additivity of the angular updates. The cummulative angular update for small batch is larger than the single step angular update for larger batch size. In fact, we can approximately observe:  $$ \\dfrac{\\Delta_t^{256}(w_t, w_{t+k})}{\\Delta_t^{k256}(w_t, w_{t+1})} \\lessapprox \\sqrt k = \\dfrac{\\sqrt{2\\eta\\lambda/(1+\\alpha)}}{\\sqrt{2k\\eta\\lambda/(1+\\alpha)}} $$\n  ","date":1596298756,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596298756,"objectID":"498ec3b21655a0c76a16d0599276b714","permalink":"https://www.javiersagastuy.com/review/200724_spherical-motion-dynamics/","publishdate":"2020-08-12T09:19:16-07:00","relpermalink":"/review/200724_spherical-motion-dynamics/","section":"review","summary":"Review Summary  Batch normalization induces scale invariance of the loss with respect to the weights (i.e. $L(x; \\theta) = L(x; k \\theta)$, for parameters $\\theta$ with BN. This expression is not mathematically precise).","tags":["Learning Dynamics","Theory"],"title":"Spherical Motion Dynamics of Deep Neural Networks with Batch Normalization and Weight Decay","type":"review"},{"authors":["Daniel Kunin","Aran Nayebi","Javier Sagastuy-Brena","Surya Ganguli","Jon Bloom","Daniel LK Yamins"],"categories":null,"content":"","date":1583020800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583020800,"objectID":"f759b3e069d311e023061c62ef9f613b","permalink":"https://www.javiersagastuy.com/publication/kunin-2020-two/","publishdate":"2020-07-09T20:23:32.256661Z","relpermalink":"/publication/kunin-2020-two/","section":"publication","summary":"The neural plausibility of backpropagation has long been disputed, primarily for its use of non-local weight transport - the biologically dubious requirement that one neuron instantaneously measure the synaptic weights of another. Until recently, attempts to create local learning rules that avoid weight transport have typically failed in the large-scale learning scenarios where backpropagation shines, e.g. ImageNet categorization with deep convolutional networks. Here, we investigate a recently proposed local learning rule that yields competitive performance with backpropagation and find that it is highly sensitive to metaparameter choices, requiring laborious tuning that does not transfer across network architecture. Our analysis indicates the underlying mathematical reason for this instability, allowing us to identify a more robust local learning rule that better transfers without metaparameter tuning. Nonetheless, we find a performance and stability gap between this local rule and backpropagation that widens with increasing model depth. We then investigate several non-local learning rules that relax the need for instantaneous weight transport into a more biologically-plausible 'weight estimation' process, showing that these rules match state-of-the-art performance on deep networks and operate effectively in the presence of noisy updates. Taken together, our results suggest two routes towards the discovery of neural implementations for credit assignment without weight symmetry: further improvement of local rules so that they perform consistently across architectures and the identification of biological implementations for non-local learning mechanisms.","tags":["ICML","Learning Rules","Deep Learning"],"title":"Two Routes to Scalable Credit Assignment without Weight Symmetry","type":"publication"}]