<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Theory | Javier Sagastuy</title>
    <link>https://www.javiersagastuy.com/tag/theory/</link>
      <atom:link href="https://www.javiersagastuy.com/tag/theory/index.xml" rel="self" type="application/rss+xml" />
    <description>Theory</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Fri, 28 Aug 2020 19:19:16 -0700</lastBuildDate>
    <image>
      <url>https://www.javiersagastuy.com/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>Theory</title>
      <link>https://www.javiersagastuy.com/tag/theory/</link>
    </image>
    
    <item>
      <title>Stochastic gradient descent performs variational inference, converges to limit cycles for deep networks</title>
      <link>https://www.javiersagastuy.com/review/200828_sg-performs-variational-inference-limit-cycles/</link>
      <pubDate>Fri, 28 Aug 2020 19:19:16 -0700</pubDate>
      <guid>https://www.javiersagastuy.com/review/200828_sg-performs-variational-inference-limit-cycles/</guid>
      <description>&lt;h3 id=&#34;review&#34;&gt;Review&lt;/h3&gt;
&lt;h4 id=&#34;summary&#34;&gt;Summary&lt;/h4&gt;
&lt;h5 id=&#34;variantional-inference&#34;&gt;Variantional inference&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;Proof that SGD minimizes a potential along with an entropic regularization term.&lt;/li&gt;
&lt;li&gt;However, this potential differs from the loss used to compute backpropagation gradients. They are only equal if the gradient noise were isotropic (i.e. a constant multiple of the identity).&lt;/li&gt;
&lt;li&gt;The gradient noise is highly non-isotropic with a very low rank covariance matrix.&lt;/li&gt;
&lt;li&gt;The locations in weight space to which SGD &amp;ldquo;converges&amp;rdquo; are not true local minima of the original loss:&lt;/li&gt;
&lt;li&gt;Their SDE analysis does not make the same assumptions as previous papers (e.g. quadratic approximation of the loss), even though the SDE they analyze is the same as in the papers I recently reviewed (albeit with different notation). Instead, they show how the continuous-time limit of SGD evolves according to the Fokker-Planck equation.
&lt;ul&gt;
&lt;li&gt;No quadratic assumption on loss&lt;/li&gt;
&lt;li&gt;No constant covariance (thought experiments show it does not change much)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;The practical implications of their analysis are:
&lt;ul&gt;
&lt;li&gt;The LR should scale linearly with BS to generalize well.&lt;/li&gt;
&lt;li&gt;Sampling with replacement is better than without replacement, since the diffusion matrix (i.e. correlation matrix), with a more prononced effect at larger batches.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;limit-cycles&#34;&gt;Limit Cycles&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;In a vicinity of a critical point, the weights do not present Brownian motion, rather they have a deterministic component generating loops in weight space.&lt;/li&gt;
&lt;li&gt;Relevant observables shown:
&lt;ul&gt;
&lt;li&gt;Eigenvalue spectra of covariance matrices over time (does not change much), along with mean and std of eigenvalues, as well as rank. These quantities inform the performance of a given architecture on a dataset.&lt;/li&gt;
&lt;li&gt;Fast Fourier Transform of weights over time (should be constant in Brownian motion case).&lt;/li&gt;
&lt;li&gt;Auto-correlation of weights (should quickly decay to zero in Brownian motion case).&lt;/li&gt;
&lt;li&gt;Dimension-normalized gradient norm: they observe it does not decrease wrt the number of epochs and even shows a growing trend. This is justified by the presence of a non-zero force $j(x)$ which is responsible for theobserved Fourier harmonics.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>A Variational Analysis of Stochastic Gradient Algorithms</title>
      <link>https://www.javiersagastuy.com/review/200818_variational-analysis-sgd/</link>
      <pubDate>Tue, 18 Aug 2020 09:19:16 -0700</pubDate>
      <guid>https://www.javiersagastuy.com/review/200818_variational-analysis-sgd/</guid>
      <description>&lt;h3 id=&#34;review&#34;&gt;Review&lt;/h3&gt;
&lt;h4 id=&#34;summary&#34;&gt;Summary&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;The authors expand on their 
&lt;a href=&#34;https://opt-ml.org/papers/OPT2015_paper_8.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;previous work&lt;/a&gt; on the continuous time-limit of SGD. They show how SGD with a constant LR can be modelled as an SDE that reaches a stationary distribution. Further, if the loss function is interpreted as a log-posterior and make the goal to approximate the posterior as well as possible (in terms of KL divergence), the authors derive optimal hyperparameters.&lt;/li&gt;
&lt;li&gt;4 key assumptions:
&lt;ul&gt;
&lt;li&gt;The gradient noise is Gaussian&lt;/li&gt;
&lt;li&gt;The weight estimates are constrained to a small enough region of parameter space that the sampling noise covariance of the stochastic gradients is constant.&lt;/li&gt;
&lt;li&gt;The step size is small enough that the discrete Markov Process can be approximated by a continuous time Markov process.&lt;/li&gt;
&lt;li&gt;Lastly, the stationary distribution of the process has a closed form solution if we assume we can approximate the loss as a quadratic bowl (we are close to a local minima).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;The authors show how this SDE view is related to stochastic gradient Fisher scoring.&lt;/li&gt;
&lt;li&gt;The main practical contribution, is that this view provides a way to compute mathematically optimal hyperparameters for a small amount if data, in contrast to the computational burden of cross-validating parameters.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Three Factors Influencing Minima in SGD</title>
      <link>https://www.javiersagastuy.com/review/200812_three-factors-influencing-minima/</link>
      <pubDate>Wed, 12 Aug 2020 09:19:16 -0700</pubDate>
      <guid>https://www.javiersagastuy.com/review/200812_three-factors-influencing-minima/</guid>
      <description>&lt;h3 id=&#34;review&#34;&gt;Review&lt;/h3&gt;
&lt;h4 id=&#34;summary&#34;&gt;Summary&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;SGD performs similarly for different batch sizes, but a constant LR/BS ratio.&lt;/li&gt;
&lt;li&gt;The authors note that SGD with the same LR/BS ratio are different discretizations of the same Stochastic Differential Equation.&lt;/li&gt;
&lt;li&gt;LR schedules and BS schedules are interchangeable, what matters, again, is what the LR/BS looks like.&lt;/li&gt;
&lt;li&gt;Width of minima is defined in terms of the trace of the Hessian $Tr(H)$ at the minima: lower trace = wider minima.
&lt;ul&gt;
&lt;li&gt;Assumption 1: At a local minima, loss surface approximated via a quadratic bowl. This lets training process be approximated by Orenstein-Unhlenbcek process.&lt;/li&gt;
&lt;li&gt;Assumption 2: $H$ is approximated via the covariance matrix of the stochastic gradients ($H=C$ relies on $C$ being anisotropic).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Larger LR/BS correlates with wider minima, giving better generalization.&lt;/li&gt;
&lt;li&gt;However, larger $\beta$, with constant $\frac{LR}{BS}=\frac{\beta \eta}{\beta S}$ causes the approximation to the SDE to break down, leading to lower performance.&lt;/li&gt;
&lt;li&gt;Discretization errors become aparent at large learning rates.&lt;/li&gt;
&lt;li&gt;Central Limit Theorem assumptions break down for small dataset, large batches.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Spherical Motion Dynamics of Deep Neural Networks with Batch Normalization and Weight Decay</title>
      <link>https://www.javiersagastuy.com/review/200724_spherical-motion-dynamics/</link>
      <pubDate>Sat, 01 Aug 2020 09:19:16 -0700</pubDate>
      <guid>https://www.javiersagastuy.com/review/200724_spherical-motion-dynamics/</guid>
      <description>&lt;h3 id=&#34;review&#34;&gt;Review&lt;/h3&gt;
&lt;h4 id=&#34;summary&#34;&gt;Summary&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Batch normalization induces scale invariance of the loss with respect to the weights (i.e. $L(x; \theta) = L(x; k \theta)$, for parameters $\theta$ with BN. This expression is not mathematically precise).&lt;/li&gt;
&lt;li&gt;The scale invariance implies in $&amp;lt;\theta_t, \partial L / \partial \theta |_{\theta = \theta_t}&amp;gt; = 0$&lt;/li&gt;
&lt;li&gt;A DNN trained with BN and WD using SDGM, with WD $\lambda$, Momentum $\alpha \geq 0$ and LR $\eta$ has an angular update $\Delta_t$ satisfying
$$
\lim_{t\rightarrow \infty}\Delta_t \simeq \sqrt{\dfrac{2\eta\lambda}{1+\alpha}}
$$&lt;/li&gt;
&lt;li&gt;At equilibrium, the &amp;ldquo;force&amp;rdquo; pulling the weights towards the origin (from the weight decay term) and the &amp;ldquo;force&amp;rdquo; pushing the weights orthogonally balance out, resulting in movement on the surface of a sphere, with a fixed angular update at every step in training.&lt;/li&gt;
&lt;li&gt;Properties of Spherical Motion Dynamics:
&lt;ul&gt;
&lt;li&gt;Scale-invariant weights do not suffer from vanishing or exploding gradients.&lt;/li&gt;
&lt;li&gt;SGDM cannot get trapped in local minima, since it guarantees an angular update size on $\mathcal{S}^{p-1}$.&lt;/li&gt;
&lt;li&gt;With SGDM weights will not converge unless the LR $\eta$ is manually decreased.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;comments&#34;&gt;Comments&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The theory shown does not seem to consider the role of the batch size in the stochasticity of the training process.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The authors&amp;rsquo; discussion on the Linear Scaling Principle (rescaling the learning rate with the batch size) is not entirely clear to me. It seems the main takeaway is that when rescaling the LR by a factor of $k$ to use a batch size that is $k$ times larger, then the angular update is affected mainly by the change in the rescaling of the LR (which scales with $\sqrt k$).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The authors compare the angular update between $w_t$ and $w_{t+1}$ for the larger batch size against the cumulative angular update over $k$ steps for the smaller batch.  We interpret this to mean the angular update between $w_t$ and $w_{t+k}$, since we cannot say anything about the additivity of the angular updates.&lt;/li&gt;
&lt;li&gt;The cummulative angular update for small batch is larger than the single step angular update for larger batch size. In fact, we can approximately observe:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
\dfrac{\Delta_t^{256}(w_t, w_{t+k})}{\Delta_t^{k256}(w_t, w_{t+1})} \lessapprox
\sqrt k = \dfrac{\sqrt{2\eta\lambda/(1+\alpha)}}{\sqrt{2k\eta\lambda/(1+\alpha)}}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
