<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>SGD | Javier Sagastuy</title>
    <link>https://www.javiersagastuy.com/tag/sgd/</link>
      <atom:link href="https://www.javiersagastuy.com/tag/sgd/index.xml" rel="self" type="application/rss+xml" />
    <description>SGD</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Wed, 12 Aug 2020 09:19:16 -0700</lastBuildDate>
    <image>
      <url>https://www.javiersagastuy.com/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>SGD</title>
      <link>https://www.javiersagastuy.com/tag/sgd/</link>
    </image>
    
    <item>
      <title>Three Factors Influencing Minima in SGD</title>
      <link>https://www.javiersagastuy.com/review/200812_three-factors-influencing-minima/</link>
      <pubDate>Wed, 12 Aug 2020 09:19:16 -0700</pubDate>
      <guid>https://www.javiersagastuy.com/review/200812_three-factors-influencing-minima/</guid>
      <description>&lt;h3 id=&#34;review&#34;&gt;Review&lt;/h3&gt;
&lt;h4 id=&#34;summary&#34;&gt;Summary&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;SGD performs similarly for different batch sizes, but a constant LR/BS ratio.&lt;/li&gt;
&lt;li&gt;The authors note that SGD with the same LR/BS ratio are different discretizations of the same Stochastic Differential Equation.&lt;/li&gt;
&lt;li&gt;LR schedules and BS schedules are interchangeable, what matters, again, is what the LR/BS looks like.&lt;/li&gt;
&lt;li&gt;Width of minima is defined in terms of the trace of the Hessian $Tr(H)$ at the minima: lower trace = wider minima.
&lt;ul&gt;
&lt;li&gt;Assumption 1: At a local minima, loss surface approximated via a quadratic bowl. This lets training process be approximated by Orenstein-Unhlenbcek process.&lt;/li&gt;
&lt;li&gt;Assumption 2: $H$ is approximated via the covariance matrix of the stochastic gradients ($H=C$ relies on $C$ being anisotropic).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Larger LR/BS correlates with wider minima, giving better generalization.&lt;/li&gt;
&lt;li&gt;However, larger $\beta$, with constant $\frac{LR}{BS}=\frac{\beta \eta}{\beta S}$ causes the approximation to the SDE to break down, leading to lower performance.&lt;/li&gt;
&lt;li&gt;Discretization errors become aparent at large learning rates.&lt;/li&gt;
&lt;li&gt;Central Limit Theorem assumptions break down for small dataset, large batches.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
